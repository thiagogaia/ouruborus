# Proposta: Busca SemÃ¢ntica para Skills do Engram

**Data:** 2026-02-03
**Status:** Proposta (nÃ£o implementado)
**Prioridade:** MÃ©dia

---

## 1. Problema Atual

Os skills sÃ£o ativados por **trigger words** â€” palavras-chave explÃ­citas na description:

```yaml
---
name: prisma-workflow
description: PadrÃµes de Prisma ORM. Use quando trabalhar com migrations,
  queries, schema design, ou relacionamentos no banco de dados.
---
```

### LimitaÃ§Ãµes

| Query do Dev | Trigger Words | Resultado |
|--------------|---------------|-----------|
| "criar migration para users" | âœ… "migration" | Ativa corretamente |
| "otimizar busca de usuÃ¡rios" | âŒ sem match | Pode nÃ£o ativar |
| "melhorar performance do banco" | âŒ sem "banco" literal | Pode nÃ£o ativar |
| "como fazer join entre tabelas" | âŒ sem match | NÃ£o ativa |

O sistema nÃ£o entende **sinÃ´nimos** nem **contexto semÃ¢ntico**.

---

## 2. SoluÃ§Ã£o Proposta: Embeddings Vetoriais

### O que sÃ£o Embeddings?

Embeddings sÃ£o representaÃ§Ãµes numÃ©ricas (vetores) de texto que capturam o **significado semÃ¢ntico**. Textos com significados similares ficam prÃ³ximos no espaÃ§o vetorial.

```
"otimizar queries de banco"     â†’ [0.23, -0.41, 0.89, ...]
"melhorar performance do DB"    â†’ [0.21, -0.39, 0.91, ...]  â† similar!
"fazer deploy na AWS"           â†’ [-0.15, 0.67, -0.22, ...] â† diferente
```

### Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BUSCA SEMÃ‚NTICA DE SKILLS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INDEXAÃ‡ÃƒO (uma vez, no /init-engram ou quando skill muda)     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Para cada skill em .claude/skills/:                    â”‚     â”‚
â”‚  â”‚   1. LÃª SKILL.md (description + body)                 â”‚     â”‚
â”‚  â”‚   2. Gera embedding (vetor de 384-1536 dimensÃµes)     â”‚     â”‚
â”‚  â”‚   3. Salva em .claude/semantic-index.json             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚  BUSCA (a cada tarefa do dev)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Dev: "otimizar a busca de usuÃ¡rios por email"         â”‚     â”‚
â”‚  â”‚   1. Gera embedding da query                          â”‚     â”‚
â”‚  â”‚   2. Calcula similaridade coseno com cada skill       â”‚     â”‚
â”‚  â”‚   3. Retorna top-3 skills mais similares              â”‚     â”‚
â”‚  â”‚   â†’ prisma-workflow (0.87), db-expert (0.82)          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. OpÃ§Ãµes de ImplementaÃ§Ã£o

### OpÃ§Ã£o A: API de Embeddings (Cloud)

Usar APIs como OpenAI, Cohere, Voyage AI, Google.

| Provider | Modelo | DimensÃµes | Custo |
|----------|--------|-----------|-------|
| OpenAI | text-embedding-3-small | 1536 | $0.02/1M tokens |
| OpenAI | text-embedding-3-large | 3072 | $0.13/1M tokens |
| Cohere | embed-english-v3.0 | 1024 | $0.10/1M tokens |
| Voyage AI | voyage-2 | 1024 | $0.10/1M tokens |
| Google | text-embedding-004 | 768 | $0.025/1M tokens |

**PrÃ³s:**
- Alta qualidade de embeddings
- Sem necessidade de hardware
- Sempre atualizado

**Contras:**
- Custo recorrente (mesmo que baixo)
- Requer internet
- LatÃªncia de rede (+100-300ms)
- DependÃªncia de terceiros

---

### OpÃ§Ã£o B: Modelo Local (Recomendada)

Usar modelos de embedding que rodam **localmente**, sem API externa.

#### Modelos Locais Recomendados

| Modelo | Tamanho | DimensÃµes | Qualidade | Velocidade |
|--------|---------|-----------|-----------|------------|
| **all-MiniLM-L6-v2** | 80MB | 384 | Boa | Muito rÃ¡pida |
| all-mpnet-base-v2 | 420MB | 768 | Muito boa | RÃ¡pida |
| bge-small-en-v1.5 | 130MB | 384 | Muito boa | Muito rÃ¡pida |
| **nomic-embed-text-v1** | 274MB | 768 | Excelente | RÃ¡pida |
| e5-small-v2 | 130MB | 384 | Boa | Muito rÃ¡pida |

**RecomendaÃ§Ã£o:** `all-MiniLM-L6-v2` ou `bge-small-en-v1.5`
- Apenas 80-130MB
- Roda em CPU (nÃ£o precisa GPU)
- Qualidade suficiente para ~50 skills
- LatÃªncia <50ms por query

**PrÃ³s:**
- Custo ZERO apÃ³s download
- Funciona offline
- Sem dependÃªncia externa
- Privacidade total
- LatÃªncia baixa

**Contras:**
- Download inicial do modelo
- Usa ~100-500MB de RAM
- Qualidade ligeiramente inferior a APIs

---

### OpÃ§Ã£o C: TF-IDF Local (Mais Simples)

Usar algoritmo clÃ¡ssico de Information Retrieval, sem redes neurais.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Cria Ã­ndice
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
tfidf_matrix = vectorizer.fit_transform(skill_texts)

# Busca
query_vec = vectorizer.transform([query])
similarities = cosine_similarity(query_vec, tfidf_matrix)
```

**PrÃ³s:**
- Extremamente simples
- Zero dependÃªncias alÃ©m de sklearn
- Muito rÃ¡pido
- Funciona offline

**Contras:**
- NÃ£o entende sinÃ´nimos ("DB" â‰  "banco de dados")
- NÃ£o entende contexto
- Qualidade inferior a embeddings

---

## 4. RecomendaÃ§Ã£o: Modelo Local com Sentence-Transformers

### Por quÃª?

1. **Custo zero** â€” Nenhuma API, roda local
2. **Offline** â€” Funciona sem internet
3. **Qualidade** â€” Modelos pequenos jÃ¡ sÃ£o muito bons para ~50 skills
4. **Velocidade** â€” <50ms por busca em CPU
5. **Privacidade** â€” CÃ³digo nunca sai da mÃ¡quina

### DependÃªncias

```bash
pip install sentence-transformers numpy
# Download automÃ¡tico do modelo na primeira execuÃ§Ã£o (~80MB)
```

### Estrutura de Arquivos

```
.claude/
â”œâ”€â”€ skills/
â”‚   â”œâ”€â”€ prisma-workflow/
â”‚   â”œâ”€â”€ nextjs-patterns/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ semantic-index.json      # Ãndice de embeddings
â””â”€â”€ ...

core/genesis/scripts/
â”œâ”€â”€ semantic_index.py        # Gera Ã­ndice
â””â”€â”€ semantic_search.py       # Busca por similaridade
```

---

## 5. ImplementaÃ§Ã£o Proposta

### 5.1 Script: `semantic_index.py`

```python
#!/usr/bin/env python3
"""
Engram â€” Semantic Index Builder
Gera embeddings locais para todos os skills usando sentence-transformers.

Usage:
    python3 semantic_index.py --project-dir .
    python3 semantic_index.py --project-dir . --model all-MiniLM-L6-v2
"""

import argparse
import hashlib
import json
import os
from datetime import datetime
from pathlib import Path

# Lazy import para nÃ£o falhar se nÃ£o instalado
def get_model(model_name: str):
    try:
        from sentence_transformers import SentenceTransformer
        return SentenceTransformer(model_name)
    except ImportError:
        print("âŒ sentence-transformers nÃ£o instalado.")
        print("   Instale com: pip install sentence-transformers")
        raise SystemExit(1)


def extract_skill_text(skill_path: Path) -> str:
    """Extrai texto relevante do SKILL.md para embedding."""
    skill_md = skill_path / "SKILL.md"
    if not skill_md.exists():
        return ""

    content = skill_md.read_text()

    # Extrai description do frontmatter
    description = ""
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            for line in parts[1].split("\n"):
                if line.strip().startswith("description:"):
                    description = line.split(":", 1)[1].strip()
                    break
            body = parts[2].strip()
        else:
            body = content
    else:
        body = content

    # Combina description + primeiras 500 palavras do body
    body_words = " ".join(body.split()[:500])
    return f"{description}\n\n{body_words}"


def compute_hash(text: str) -> str:
    """Hash para detectar mudanÃ§as no skill."""
    return hashlib.md5(text.encode()).hexdigest()[:12]


def build_index(project_dir: str, model_name: str = "all-MiniLM-L6-v2") -> dict:
    """ConstrÃ³i Ã­ndice semÃ¢ntico de todos os skills."""
    skills_dir = Path(project_dir) / ".claude" / "skills"

    if not skills_dir.exists():
        print(f"âŒ DiretÃ³rio de skills nÃ£o encontrado: {skills_dir}")
        raise SystemExit(1)

    print(f"ğŸ” Carregando modelo: {model_name}")
    model = get_model(model_name)

    index = {
        "model": model_name,
        "dimensions": model.get_sentence_embedding_dimension(),
        "indexed_at": datetime.now().isoformat(),
        "skills": {}
    }

    # Coleta textos de todos os skills
    skill_data = []
    for skill_path in sorted(skills_dir.iterdir()):
        if not skill_path.is_dir():
            continue
        text = extract_skill_text(skill_path)
        if text:
            skill_data.append({
                "name": skill_path.name,
                "text": text,
                "hash": compute_hash(text)
            })

    if not skill_data:
        print("âš ï¸  Nenhum skill encontrado para indexar.")
        return index

    print(f"ğŸ“Š Indexando {len(skill_data)} skills...")

    # Gera embeddings em batch (mais eficiente)
    texts = [s["text"] for s in skill_data]
    embeddings = model.encode(texts, show_progress_bar=True)

    for i, skill in enumerate(skill_data):
        index["skills"][skill["name"]] = {
            "embedding": embeddings[i].tolist(),
            "hash": skill["hash"]
        }

    return index


def save_index(project_dir: str, index: dict):
    """Salva Ã­ndice em arquivo JSON."""
    index_path = Path(project_dir) / ".claude" / "semantic-index.json"
    with open(index_path, "w") as f:
        json.dump(index, f, indent=2)
    print(f"âœ… Ãndice salvo: {index_path}")
    print(f"   â€¢ {len(index['skills'])} skills indexados")
    print(f"   â€¢ {index['dimensions']} dimensÃµes")
    print(f"   â€¢ Modelo: {index['model']}")


def main():
    parser = argparse.ArgumentParser(description="Engram Semantic Index Builder")
    parser.add_argument("--project-dir", default=".", help="DiretÃ³rio do projeto")
    parser.add_argument("--model", default="all-MiniLM-L6-v2",
                        help="Modelo de embedding (default: all-MiniLM-L6-v2)")
    args = parser.parse_args()

    index = build_index(args.project_dir, args.model)
    save_index(args.project_dir, index)


if __name__ == "__main__":
    main()
```

### 5.2 Script: `semantic_search.py`

```python
#!/usr/bin/env python3
"""
Engram â€” Semantic Skill Search
Busca skills por similaridade semÃ¢ntica.

Usage:
    python3 semantic_search.py --query "otimizar queries de banco" --project-dir .
    python3 semantic_search.py --query "como fazer deploy" --top-k 5 --project-dir .
"""

import argparse
import json
import os
import sys
from pathlib import Path

import numpy as np


def load_index(project_dir: str) -> dict:
    """Carrega Ã­ndice semÃ¢ntico."""
    index_path = Path(project_dir) / ".claude" / "semantic-index.json"
    if not index_path.exists():
        print(f"âŒ Ãndice nÃ£o encontrado: {index_path}")
        print("   Execute primeiro: python3 semantic_index.py --project-dir .")
        sys.exit(1)

    with open(index_path) as f:
        return json.load(f)


def get_query_embedding(query: str, model_name: str) -> np.ndarray:
    """Gera embedding para a query."""
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(model_name)
        return model.encode(query)
    except ImportError:
        print("âŒ sentence-transformers nÃ£o instalado.")
        sys.exit(1)


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Similaridade coseno entre dois vetores."""
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def search(query: str, index: dict, top_k: int = 3) -> list[dict]:
    """Busca skills mais similares Ã  query."""
    query_emb = get_query_embedding(query, index["model"])

    results = []
    for skill_name, data in index["skills"].items():
        skill_emb = np.array(data["embedding"])
        similarity = cosine_similarity(query_emb, skill_emb)
        results.append({
            "skill": skill_name,
            "similarity": round(similarity, 3)
        })

    # Ordena por similaridade decrescente
    results.sort(key=lambda x: x["similarity"], reverse=True)
    return results[:top_k]


def main():
    parser = argparse.ArgumentParser(description="Engram Semantic Skill Search")
    parser.add_argument("--query", required=True, help="Texto para buscar")
    parser.add_argument("--project-dir", default=".", help="DiretÃ³rio do projeto")
    parser.add_argument("--top-k", type=int, default=3, help="NÃºmero de resultados")
    parser.add_argument("--json", action="store_true", help="Output em JSON")
    parser.add_argument("--threshold", type=float, default=0.3,
                        help="Similaridade mÃ­nima (0-1)")
    args = parser.parse_args()

    index = load_index(args.project_dir)
    results = search(args.query, index, args.top_k)

    # Filtra por threshold
    results = [r for r in results if r["similarity"] >= args.threshold]

    if args.json:
        print(json.dumps(results, indent=2))
    else:
        print(f"\nğŸ” Query: \"{args.query}\"")
        print(f"{'â”€' * 50}")
        if results:
            for i, r in enumerate(results, 1):
                bar = "â–ˆ" * int(r["similarity"] * 20)
                print(f"  {i}. {r['skill']:<25} {r['similarity']:.3f} {bar}")
        else:
            print("  Nenhum skill encontrado acima do threshold.")
        print()


if __name__ == "__main__":
    main()
```

---

## 6. IntegraÃ§Ã£o com Engram

### 6.1 Quando Indexar

1. **No `/init-engram`** â€” Indexa todos os skills gerados
2. **No `/create` ou `/spawn`** â€” Re-indexa apÃ³s criar novo skill
3. **No `/learn`** â€” Re-indexa se detectar skills modificados

### 6.2 Quando Buscar

No inÃ­cio de cada tarefa, o Claude pode:

```python
# PseudocÃ³digo no fluxo do Claude
def handle_task(user_query):
    # 1. Busca semÃ¢ntica
    relevant_skills = semantic_search(user_query, top_k=3, threshold=0.5)

    # 2. Carrega skills relevantes
    for skill in relevant_skills:
        load_skill(skill["name"])

    # 3. Executa tarefa com contexto dos skills
    execute_task(user_query)
```

### 6.3 Command: `/semantic`

Novo command opcional para busca explÃ­cita:

```markdown
# /semantic

Buscar skills por similaridade semÃ¢ntica.

## Uso
/semantic [query]

## Exemplo
/semantic como otimizar queries do banco de dados

## Output
ğŸ” Query: "como otimizar queries do banco de dados"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. prisma-workflow           0.847 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  2. db-expert                 0.812 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  3. code-reviewer             0.523 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

---

## 7. ComparaÃ§Ã£o de Custos

| Abordagem | Custo Inicial | Custo por Query | Custo Mensal (1000 queries) |
|-----------|---------------|-----------------|----------------------------|
| **API OpenAI** | $0 | ~$0.0001 | ~$0.10 |
| **Modelo Local** | $0 (download 80MB) | $0 | **$0** |
| **TF-IDF** | $0 | $0 | $0 |

### Recursos de Hardware (Modelo Local)

| Recurso | Requisito |
|---------|-----------|
| RAM | +100-200MB durante indexaÃ§Ã£o |
| Disco | 80-130MB para o modelo |
| CPU | Qualquer (nÃ£o precisa GPU) |
| Tempo indexaÃ§Ã£o | ~1-2s para 50 skills |
| Tempo busca | ~30-50ms por query |

---

## 8. Roadmap de ImplementaÃ§Ã£o

### Fase 1: FundaÃ§Ã£o
1. Criar `semantic_index.py`
2. Criar `semantic_search.py`
3. Adicionar ao `requirements.txt`: `sentence-transformers>=2.2.0`
4. Testar com skills existentes

### Fase 2: IntegraÃ§Ã£o
5. Integrar no `/init-engram` (indexaÃ§Ã£o automÃ¡tica)
6. Integrar no `/create` e `/spawn` (re-indexaÃ§Ã£o)
7. Criar command `/semantic` (busca explÃ­cita)

### Fase 3: AutomaÃ§Ã£o
8. Claude usa busca semÃ¢ntica automaticamente
9. Fallback para trigger words se Ã­ndice nÃ£o existir
10. MÃ©tricas de qualidade das buscas

---

## 9. ConclusÃ£o

A **OpÃ§Ã£o B (Modelo Local)** Ã© a recomendada porque:

- **Zero custo** apÃ³s instalaÃ§Ã£o
- **Funciona offline**
- **Qualidade suficiente** para o caso de uso
- **Simples** de implementar e manter
- **Privado** â€” cÃ³digo nunca sai da mÃ¡quina

O modelo `all-MiniLM-L6-v2` com apenas 80MB oferece qualidade excelente para buscar entre ~50-100 skills, com latÃªncia <50ms.

NÃ£o Ã© uma "rede neural treinada do zero" â€” Ã© um modelo **prÃ©-treinado** que apenas usamos para gerar vetores. O "aprendizado" jÃ¡ foi feito pelos criadores do modelo em datasets massivos de texto. NÃ³s apenas aplicamos.
